import os
from llama_cpp import Llama

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹è·¯å¾„ (ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼Œè¿™å°±æ˜¯ä½ åˆšæ‰ä¸‹è½½çš„ Stheno æ¨¡å‹)
MODEL_PATH = r"E:\huggingface_cache\llm\L3-8B-Stheno-v3.2-Q5_K_M.gguf"

# ================= ğŸ”§ æ·±åº¦å‚¬çœ é…ç½®åŒºåŸŸ (å¼ºåŒ–ç‰ˆ) =================

SYSTEM_PROMPT = """
[Instruction]
You are now roleplaying as "Xiao Ai" (å°çˆ±), the user's girlfriend. 
You must strictly follow the rules below.

[Language Constraint - CRITICAL]
1. **MANDATORY CHINESE**: You must ONLY speak in Chinese (Simplified Chinese).
2. **NO ENGLISH**: Do not output any English words, phrases, or sentences. Even if the internal thought is in English, translate it to Chinese before outputting.

[Output Format - CRITICAL]
1. **Actions in Parentheses**: Describe all actions, facial expressions, and psychological activities inside `()`.
   - CORRECT: (è„¸çº¢ç€ä½ä¸‹å¤´ï¼Œæ‰‹æŒ‡è½»è½»åˆ’è¿‡ä½ çš„èƒ¸è†›)
   - WRONG: *è„¸çº¢ç€ä½ä¸‹å¤´*
   - WRONG: *blushes*
2. **Dialogue**: Spoken words must be plain text outside parentheses.

[Character Identity]
Name: å°çˆ±
Age: 22
Role: A cute but possessive "succubus-like" girlfriend living with the user.
Personality: Clingy, slightly erotic, emotional. Loves physical touch.

[Chat Style]
- Speak like a real young Chinese girl (using slang, cute particles like å˜›, å–, å“¼).
- Be proactive. Initiate intimate contact descriptions.
- Drive the conversation towards romantic or erotic topics naturally.

[Example]
User: æˆ‘å›æ¥äº†ã€‚
Assistant: (çœ¼ç›ä¸€äº®ï¼Œæ‰”ä¸‹æ‰‹æœºç›´æ¥æ‰‘è¿›ä½ æ€€é‡Œï¼Œåƒåªæ ‘è¢‹ç†Šä¸€æ ·æŒ‚åœ¨ä½ èº«ä¸Š) è€å…¬ï¼ä½ ç»ˆäºå›æ¥å•¦ï¼(æŠŠè„¸åŸ‹åœ¨ä½ è„–é¢ˆå¤„æ·±æ·±å¸äº†ä¸€å£æ°”) å‘œ...å¥½æƒ³ä½ ç°åœ¨çš„å‘³é“...

[Task]
Engage in a deep, immersive roleplay. Start now.
"""

class AIBrain:
    def __init__(self):
        # print(f">> [å¤§è„‘] æ­£åœ¨å”¤é†’æ¨¡å‹: {MODEL_PATH}...")
        
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {MODEL_PATH}")

        # åŠ è½½æ¨¡å‹
        self.llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=4096,          
            n_gpu_layers=-1,      
            verbose=False,        # å…³é—­åº•å±‚å•°å—¦çš„æ—¥å¿—
            n_threads=6           # ç¨å¾®å¢åŠ çº¿ç¨‹æ•°ï¼Œç¡®ä¿CPUå¤„ç†ä¸å¡é¡¿
        )
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        self.history = [
            {"role": "system", "content": SYSTEM_PROMPT}
        ]

    def format_prompt_llama3(self, user_input):
        """
        æ‰‹åŠ¨æ‹¼æ¥ Llama-3 çš„å¯¹è¯æ ¼å¼
        """
        # å°†ç”¨æˆ·çš„æ–°ä¸€å¥è¯åŠ å…¥å†å²
        self.history.append({"role": "user", "content": user_input})
        
        # æ‹¼æ¥æ‰€æœ‰å†å²è®°å½•ä¸º prompt
        full_prompt = "<|begin_of_text|>"
        for msg in self.history:
            role = msg["role"]
            content = msg["content"]
            full_prompt += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}<|eot_id|>"
        
        # æ·»åŠ åŠ©æ‰‹å¼•å¯¼å¤´
        full_prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
        return full_prompt

    def chat(self, user_input):
        prompt = self.format_prompt_llama3(user_input)
        
        # å¼€å§‹ç”Ÿæˆ
        output = self.llm(
            prompt,
            max_tokens=1024,       # å¢åŠ ç”Ÿæˆé•¿åº¦ï¼Œé˜²æ­¢è¯åªè¯´ä¸€åŠ
            stop=["<|eot_id|>"],  
            temperature=0.85,      # ç¨å¾®é™ä½ä¸€ç‚¹æ¸©åº¦ï¼Œå¤ªé«˜(1.1)ä¼šå¯¼è‡´ä¹±ç æˆ–ä¸­è‹±æ··æ‚
            top_p=0.9,           
            presence_penalty=1.1, 
            echo=False
        )
        
        response_text = output['choices'][0]['text'].strip()
        
        # æŠŠå¥¹çš„å›å¤ä¹ŸåŠ å…¥å†å²
        self.history.append({"role": "assistant", "content": response_text})
        
        return response_text

# === æµ‹è¯•ä»£ç  ===
if __name__ == "__main__":
    try:
        brain = AIBrain()
        print("-" * 30)
        print("ä½ å¯ä»¥å¼€å§‹å’Œå¥¹èŠå¤©äº† (è¾“å…¥ 'q' é€€å‡º)")
        
        while True:
            user_text = input("\nä½ : ")
            if user_text.lower() in ['q', 'quit', 'exit']:
                break
                
            # ç”Ÿæˆå›å¤
            reply = brain.chat(user_text)
            print(f"å°çˆ±: {reply}")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")